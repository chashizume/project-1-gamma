---
title: "Project Report"
author: "Team Gamma"
date: 2021-10-14
output:
  github_document:
    toc: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(gtrendsR)
library(tidyquant)
library(corrr)
library(ggcorrplot)
```

# Google Trends and the Pandemic

### Authors: Megan, Colin, Claire, Meagan, Akoua

## Introduction

In this project, our team set out to explore relationships between Google search trends and the state of the COVID-19 pandemic. Our central question was the following: "How does Google search activity correlate with the pandemic?"

## Background

### Google Trends

[Google Trends](https://trends.google.com/trends/?geo=US)[1] provides Google search data, showing a measure of interest in a keyword and topic over time. Google trends anonymizes, categorizes, and groups together data, from which we can access a representative sample. It also normalizes search data by dividing each data point by total searches of location and time it represents and scaling numbers on a range of 0 to 100.

We pulled data from Google Trends to see how people react to different moments during the pandemic and explore the magnitude of those moments. When choosing specific search terms, we looked at the visual output on the Google Trends site to see if we can discover interesting patterns before bringing the data into R

To access Google Trends data without having to manually downloading .csv files for each query, we used the R package [`gtrendsR`](https://cran.r-project.org/web/packages/gtrendsR/gtrendsR.pdf) which scrapes the Google Trends site. We used `gtrends()` with different parameters for search keyword, time, and geographical location.

### New York Times COVID-19 Cases and Census Bureau

The New York Times (NYT) has been collecting and publishing COVID case and death data since the first recorded case back in January 2020. The dataset is collected from local and state governments and health department in the Times' attempt to create a fuller picture of the pandemic. Data is reported by county and state and is publicly available via [GitHub](https://github.com/nytimes/covid-19-data).

We pair the above COVID data with population data from the [Census Bureau](https://www.census.gov/data.html), and follow the steps provided in c06. The population data can be found [here](https://data.census.gov/cedsci/table?q=United%20States&t=Population%20Total&g=0100000US%240500000&tid=ACSDT1Y2017.B01003&vintage=2017&layer=state&cid=DP05_0001E).

### Our World in Data Vaccination Rates

To look at vaccination rates, we used the Our World in Data (OWID) vaccinations dataset, found on their [GitHub](https://github.com/owid/covid-19-data/tree/master/public/data/vaccinations). This data is collected by the **Our World In Data Team** based on official vaccination reports. According to their [site](https://ourworldindata.org/covid-vaccinations), "Our vaccination dataset uses the most recent official numbers from governments and health ministries worldwide. Population estimates for per-capita metrics are based on the United Nations World Population Prospects. Income groups are based on the World Bank classification."

## Exploratory Data Analysis

### NYT Covid Cases and Census Data

To import the COVID-19 case and death data, we will start by repeating the steps outlined in c06 by importing NYT data and Census data and creating one dataset.

```{r}
# Code taken from c06; credit to Zach del Rosario, professor at Olin College

df_pop <- read_csv(
  "./data/ACSDT5Y2018.B01003_data_with_overlays_2021-10-05T112340.csv", 
  skip = 1
)

## URL for the NYT covid-19 county-level data, raw
url_counties <- "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv"
filename_nyt <- "./data/nyt_counties.csv"

## Download the data locally
curl::curl_download(
        url_counties,
        destfile = filename_nyt
      )

## Loads the downloaded csv
df_covid <- read_csv(filename_nyt)

df_pop <- df_pop %>%
  mutate(fips = str_sub(id, -5))

df_covid <- df_covid %>%
  left_join(df_pop, "fips")

## Isolate specific columns
df_data <-
  df_covid %>%
  select(
    date,
    county,
    state,
    fips,
    cases,
    deaths,
    population = `Estimate!!Total`
  )

## Calculate case and death rates per 100k people
df_normalized <- df_data %>%
  mutate(
    total_cases_per100k = cases / population * 100000,
    total_deaths_per100k = deaths / population * 100000
  )

glimpse(df_normalized)

summary(df_normalized)
```
### Dataset observations

The New York Times and the Census Bureau are very reputable sources. The NYT is sourcing their data from government and public health authorities, while the Census is notably conducted as a government means of tracking population data. 

The current combined dataset has location, population, and time-based COVID case and death data. Based on the numbers shown through the `summarise()` function, we can see that the case and death counts are cumulative, not per day, as we may be used to when looking at NYT COVID charts.

In detail, the following variables are in this combined dataset:
- `date`: Date of reported cases and deaths
- `county`: County
- `state`: State
- `fips`: FIPS code; unique to each county and state
- `cases`: Number of cumulative cases
- `deaths`: Number of cumulative deaths
- `population`: Population of given county
- `total_cases_per100k`: `cases` normalized for population size
- `total_deaths_per100k`: `deaths` normalized for population size

Like with any dataset, there are limitations. Since this data is reported and not automatically recorded by a sensor, per say, there's always room for undereporting, which is a [major concern](https://www.npr.org/2019/06/04/728034176/2020-census-could-lead-to-worst-undercount-of-black-latinx-people-in-30-years) when it comes to conducting the Census and using it to make legislative decisions [2]. There have been several moments in the last year and a half where the NYT had to update their data to fix double-counted cases, or when certain jurisdictions would report high numbers of cases or deaths after not reporting any for a few days [3]. Overall, though, the credibility of both datasets is clear, and these two sources are some of the best for using in our EDA. In short, the data isn't perfect, but it doesn't get much better than this for people-based data.

### Vaccination Data

```{r}
## URL for the OWID vaccination data
url_vax <- 
  "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv"
filename_vax <- "./data/owid_vax.csv"

## Download the data locally
curl::curl_download(
        url_vax,
        destfile = filename_vax
      )

## Loads the downloaded csv
df_vax <- read_csv(filename_vax) %>%
  filter(location == "United States") %>%
  drop_na(daily_vaccinations)

glimpse(df_vax)
summary(df_vax)
```
#### Dataset observations

One reason we turned to the OWID dataset is because Google uses this data to create their in-house visuals related to COVID vaccinations, which spoke to the credibility of the data. For the scope of our project, we're more focused on the US, but this dataset does have international data. The per-capita values are calculated "based on the United Nations World Population Prospects", according to the OWID [site](https://ourworldindata.org/covid-vaccinations).

In detail, the following variables are in this combined dataset:
- `location`, `iso_code`: filtered to be "United States" and "US".
- `date`: Date of reported vaccinations
- `total_vaccinations`: number of doses delivered. For vaccines that require multiple doses, each individual dose is counted
- `people_vaccinated`: total number of people vaccinated
- `people_fully_vaccinated`: total number of people fully vaccinated (1 J&J, 2 Moderna/Pfizer)
- `total_boosters`: total number of booster shots injected
- `daily_vaccinations_raw`: daily change in the total number of doses administered; it is only calculated for consecutive days. This is a raw measure provided for data checks and transparency; OWID recommends using `daily_vaccinations` instead
- `daily_vaccinations`: new doses administered per day (7-day smoothed)
- `total_vaccinations_per_hundred`: `total_vaccinations` per 100 people in total population of the country
- `people_vaccinated_per_hundred`: `people_vaccinated` per 100 people in total population of the country
- `people_fully_vaccinated_per_hundred`: `people_fully_vaccinated` per 100 people in total population of the country
- `total_boosters_per_hundred`: `total_boosters` per 100 people in total population of the country
- `daily_vaccinations_per_million`: `daily_vaccinations` per one million people in total population of the country    

Like with the COVID case and death data, we can assume there are going to be several days where vaccination numbers are not going to be reported, especially if many places don't distribute vaccinations on certain weekends or holidays. This could offer one explanation for the many NAs prevalent in this dataset. Because this data is likely reported from places that are actively distributing vaccines, we can assume minimal error with the exception of human miscounting. Unlike with the Census data, people who are getting vaccinated will have had to come into contact with some sort of healthcare professional in order to receive the vaccine, at which point they will likely get recorded. I think data reliability becomes more of an issue where the vaccine rollout infrastructure isn't as defined, and where vaccines are scarcer, which may be more of the case in developing countries or countries with low access to vaccines.

### Google Trends Data

We began by extracting data from Google trends and converting into a format which is useful for us. The data comes in as a  list of dataframes which describe the time-dependent, location-dependent, and related searches to the given queries. We begin by extracting the time-dependent data, replacing the "<1" elements with 0 so it can be converted to a numeric type, and selecting a data range we want to look at. We can then plot the popularity of searches over time.

```{r}

a <- gtrends(c("covid", "coronavirus", "covid-19", "covid 19"), time = "today+5-y", geo = c("US", "US", "US", "US"))
b <- a$interest_over_time %>%
  mutate(hits=replace(hits, hits=="<1", "0")) %>%
  mutate(hits = as.integer(hits)) %>%
  filter(date >= as.Date("2019-10-01"))

b %>%
  ggplot() +
  geom_line(mapping = aes(x =  as.POSIXct(date), y = hits, color = keyword)) +
  labs(x = "Date", y = "Hits", title = "Relative hits for several search queries")
```
Graphs like this can tell us what people were searching for at any particular time, like the above example which shows the term "coronavirus" jumping then dropping dramatically as it was replaced with other terms. However, this data alone does not really tell us much about COVID without including more information.

```{r}
# a <- gtrends(c("covid", "coronavirus", "covid-19", "covid 19"), time = "today+5-y", geo = c("US", "US", "US", "US"))
# b <- a$interest_over_time %>%
#   mutate(hits=replace(hits, hits=="<1", "0")) %>%
#   mutate(hits = as.integer(hits)) %>%
#   filter(date >= as.Date("2019-10-01"))

c <- df_normalized %>%
  group_by(date) %>%
  summarize(cases = sum(cases)) %>%
  mutate(dcases_dt = cases - lag(cases))
ggplot(NULL) +
  geom_line(data = c, mapping = aes(x =  as.POSIXct(date), y = dcases_dt))+
  geom_line(data = b, mapping = aes(x =  as.POSIXct(date), y = hits*3000, color = keyword)) +
  
  scale_y_continuous(
    
    name = "Daily COVID cases",
    
    sec.axis = sec_axis(~.*1/3000, name="Search Popularity")
  )

```
If we take that same data and overlay daily COVID cases, we start to get a clearer picture of what is happening. Here we can see that searches for "coronavirus" peaked very early on, before there were almost any cases in the US. The three other terms then start to emerge just before and during the beginning of the first wave in the US and all but "covid" begin to die out to fractions of their previous levels. Interestingly, the search rate for "covid" tends to track relatively well with daily coronavirus cases, with a spike roughly corresponding to each of the major spikes in the data. It is also the only term to experience any increase during the major wave in the latter part of 2021.


#### Quality of Google Data

The data from Google is aggregated over national searches and normalized by search volume and location to represent an average popularity for a specific term. Google does not publish data for all search queries, so in some cases it is necessary to use a related term or there may be no data at all. This scaling by relative proportions can often make it hard to understand the relative magnitude of searches as there is no real data which says how often a term is searched or what proportion of the population searched the term.

Google does preprocessing on this data to remove searches they suspect to be machine generated or made intentionally to affect their numbers. While we assume that this is done in a fair and unbiased manner we have now way of verifying their process as they do not disclose which specific steps they take. While this may be of concern for terms that are more likely to be spammed (election results, polling data) it likely has little if any effect for the terms we consider here.

More generally, the data represents searches on Google so likely overcounts younger and more technology-connected populations and does not necessarily capture a representative proportion of the US public. Since Google provides no demographic information on the people who search for terms we do not know to what extent this skews the data.


## Time series analysis

The data we consider here (cases, searches, vaccinations, etc.) are all time depedent and share a common time scale. Therefore it makes sense to start our analysis by looking at time dependent relationships between them. We decided to focus on two particular relationships: the effect of quarrantine on search trends and the correlation between misinformation and case load.

### Start of pandemic graphs

***TODO: Meagan and Claire***

### Misinfomation graphs

One of the most defining features of the past two years is the seemingly constant stream of new, creative, and utterly ridiculous home COVID remedies that keep popping up in the news. While to many these may seem like a crazy hoax, it is important to remember that their creation is often an act of desperation from people who feel like they have nowhere else to turn. We began studying the relationship between these searches and factors like cases, vaccinations, and deaths and found that while many terms show almost no correlation with these factors there are some important cases where they align. 


## Abstracting away from time series

### Correlation grpahs
### Anything else

## Remaining Questions

What questions do you have remaining?


<!-- -------------------------------------------------- -->

[1] More information on how Google Trends creates its normalized data can be found [here](https://support.google.com/trends/answer/4365533?hl=en&ref_topic=6248052).

[2] Census under-counting was [exacerbated by the Trump Administration and the pandemic](https://news.yahoo.com/2020-census-may-massively-undercounted-094704777.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAEEy4BmL812XqU3FmbpwVkGRnhKpWWB-qIMHFIPa-zf9kH56QFEAGeveCzFT0CDIBYR9eH1Gdbm6fmrZrhr3hIvE_2x716qUfGWKLuGvs-xYSSxASbwzL4L1d245nmIs0t-WYePjd2OckYH0snqM1Cykf2pStCkcZDU44-5G8JIU) in 2020, which will have lasting negative effects on minority Americans.

[3] The New York Times also added an [FAQ section](https://www.nytimes.com/interactive/2020/us/about-coronavirus-data-maps.html) to help explain the dataset.

